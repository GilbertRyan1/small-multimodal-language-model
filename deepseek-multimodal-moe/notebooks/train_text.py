# -*- coding: utf-8 -*-
"""Text.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xDAUuVviJEYOFtJ4LRRFxsBDKMJdp9MN
"""

# Cell 1: Install Dependencies
!pip install -q transformers datasets accelerate
!pip install -q bitsandbytes peft

# Cell 2: Import Libraries
import torch
from datasets import load_dataset, DatasetDict
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline
)
from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig
import os

# Suppress warnings
import warnings
warnings.filterwarnings("ignore")

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"Current GPU: {torch.cuda.get_device_name(0)}")

# Cell 3: Load and Subsample the BoolQ Dataset

# Define our sample sizes
TRAIN_SAMPLES = 1500
VAL_SAMPLES = 200

# 1. Load the dataset from the hub
full_dataset = load_dataset("boolq")

# 2. Calculate and print the percentage we are using
original_train_size = len(full_dataset['train'])
original_val_size = len(full_dataset['validation'])

train_percentage = (TRAIN_SAMPLES / original_train_size) * 100
val_percentage = (VAL_SAMPLES / original_val_size) * 100

print("--- Dataset Subsampling ---")
print(f"Original train size: {original_train_size}")
print(f"Using {TRAIN_SAMPLES} train samples ({train_percentage:.2f}% of original)")
print(f"Original validation size: {original_val_size}")
print(f"Using {VAL_SAMPLES} validation samples ({val_percentage:.2f}% of original)")
print("---------------------------")

# 3. Create the smaller, balanced dataset
# We shuffle before selecting to get a random subset
small_train_dataset = full_dataset['train'].shuffle(seed=42).select(range(TRAIN_SAMPLES))
small_val_dataset = full_dataset['validation'].shuffle(seed=42).select(range(VAL_SAMPLES))

# 4. Combine them into a new DatasetDict
dataset = DatasetDict({
    'train': small_train_dataset,
    'validation': small_val_dataset
})

# 5. Print the final dataset to verify
print("\nFinal processed dataset:")
print(dataset)

# 6. Print one example to see the structure
print("\nExample data point:")
print(dataset['train'][0])

# Cell 4: Load Tokenizer and Quantized Model

# 1. Define the model ID
model_id = "deepseek-ai/deepseek-moe-16b-base" #

# 2. Configure 4-bit quantization (QLoRA)
# This is essential for fitting the model in 40GB
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 as required
    bnb_4bit_use_double_quant=True,
)

# 3. Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
# Set padding token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print("Tokenizer loaded.")

# 4. Load the 4-bit quantized model
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    device_map="auto", # Automatically maps model layers to GPU
    trust_remote_code=True
)

print(f"\nModel {model_id} loaded successfully in 4-bit.")

# Cell 4 - FIX: Check GPU Memory
import torch

# Print current GPU memory allocated
allocated_gb = torch.cuda.memory_allocated(0) / (1024**3)
reserved_gb = torch.cuda.memory_reserved(0) / (1024**3)

print(f"--- GPU Memory Usage ---")
print(f"Model loaded successfully.")
print(f"Allocated: {allocated_gb:.2f} GB")
print(f"Reserved: {reserved_gb:.2f} GB")
print(f"Total GPU Memory: 40 GB")

# Cell 5: Preprocess Data and Create Prompt

# 1. Define the preprocessing function
def create_prompt(example):
    # Format the prompt
    prompt_template = f"Passage: {example['passage']}\nQuestion: {example['question']}\nAnswer: "

    # Format the answer
    answer = "Yes" if example['answer'] else "No"

    # Tokenize the prompt and the answer *separately* to know the length
    prompt_tokenized = tokenizer(prompt_template, add_special_tokens=False)
    answer_tokenized = tokenizer(answer + tokenizer.eos_token, add_special_tokens=False) # Add EOS token to signal end

    # Combine the tokenized inputs
    input_ids = prompt_tokenized['input_ids'] + answer_tokenized['input_ids']
    attention_mask = prompt_tokenized['attention_mask'] + answer_tokenized['attention_mask']

    # Create labels, masking out the prompt part
    labels = [-100] * len(prompt_tokenized['input_ids']) + answer_tokenized['input_ids']

    # Ensure inputs are not too long
    # We'll set a reasonable max length for this task
    max_length = 1024
    if len(input_ids) > max_length:
        input_ids = input_ids[:max_length]
        attention_mask = attention_mask[:max_length]
        labels = labels[:max_length]

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

# 2. Apply the preprocessing to the dataset
tokenized_dataset = dataset.map(
    create_prompt,
    remove_columns=['question', 'answer', 'passage'] # Remove old columns
)

# 3. Print an example to verify
print("--- Processed Example ---")
example = tokenized_dataset['train'][0]
print(f"Input IDs (sample): {example['input_ids'][:20]}...")
print(f"Labels (sample): {example['labels'][:20]}...")

# Decode to see the masking
print("\n--- Decoded Verification ---")
print("Decoded Input (should be prompt + answer):")
print(tokenizer.decode(example['input_ids']))

print("\nDecoded Labels (should be only answer):")
# We replace -100 with a pad token ID for decoding, so it's invisible
decoded_labels = [l if l != -100 else tokenizer.pad_token_id for l in example['labels']]
print(tokenizer.decode(decoded_labels, skip_special_tokens=True))

# Cell 6: Configure PEFT (QLoRA)

# 1. Define LoRA configuration
lora_config = LoraConfig(
    r=16,  # LoRA attention dimension (rank)
    lora_alpha=32, # Alpha parameter for scaling
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    # Target the query, key, value, and output layers
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
)

# 2. Apply LoRA to the 4-bit model
peft_model = get_peft_model(model, lora_config)

# 3. Print the number of trainable parameters
print("--- PEFT Model Configured ---")
peft_model.print_trainable_parameters()

# Cell 7: Configure Metrics, Training Arguments, and Trainer

# 1. Install helper libraries for metrics
!pip install -q evaluate scikit-learn

import numpy as np
import evaluate # Hugging Face's metrics library

# 2. Load the metrics
accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")

# 3. Define the metrics computation function
def compute_metrics(eval_preds):
    logits, labels = eval_preds

    # Get the predictions (the token with the highest logit)
    predictions = np.argmax(logits, axis=-1)

    # We need to decode the predictions and labels to compare them
    # Replace -100 in labels with pad_token_id for safe decoding
    labels[labels == -100] = tokenizer.pad_token_id

    # Decode
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Clean up the decoded strings
    # The model might output "Answer: Yes", so we just check for "Yes" or "No"
    cleaned_preds = ["Yes" if "yes" in p.lower() else "No" for p in decoded_preds]
    cleaned_labels = ["Yes" if "yes" in l.lower() else "No" for l in decoded_labels]

    # Calculate F1 and Accuracy
    try:
        acc_results = accuracy_metric.compute(predictions=cleaned_preds, references=cleaned_labels)
        f1_results = f1_metric.compute(predictions=cleaned_preds, references=cleaned_labels, average="macro", pos_label="Yes")

        return {
            "accuracy": acc_results["accuracy"],
            "f1": f1_results["f1"],
        }
    except Exception as e:
        print(f"Error computing metrics: {e}")
        print(f"Preds: {cleaned_preds[:5]}")
        print(f"Labels: {cleaned_labels[:5]}")
        return {"accuracy": 0, "f1": 0}


# Cell 7 - FIX: Correcting TrainingArguments

# Cell 7 - FIX 5: Importing Trainer

# 1. Re-define Training Arguments to avoid the conflict
training_args = TrainingArguments(
    output_dir="./results-boolq",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    gradient_checkpointing=True,
    optim="paged_adamw_8bit",
    logging_steps=10,
    save_strategy="steps",
    save_steps=50,
    do_eval=True,
    eval_steps=50,
    learning_rate=2e-4,
    bf16=True,
    report_to="none",
)

# 2. Re-instantiate the Trainer
# --- HERE IS THE FIX ---
from transformers import Trainer, DataCollatorForLanguageModeling
# --- END OF FIX ---

trainer = Trainer(
    model=peft_model,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    args=training_args,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
    compute_metrics=compute_metrics,
)

# 3. We also need to re-enable gradient checkpointing on the model
peft_model.config.use_cache = False

print("Trainer re-configured successfully. Imports are fixed.")

# Cell 7.5: Enable TensorBoard Reporting

# Update the TrainingArguments to enable TensorBoard
trainer.args.report_to = "tensorboard"

print("Trainer arguments updated.")
print(f"Reporting to: {trainer.args.report_to}")

# Cell 8 - FIX: Update Data Collator

from transformers import DataCollatorForSeq2Seq

# 1. Create a data collator that pads inputs and labels correctly
# We use padding="longest" to pad each batch to the longest sequence in that batch
# This is more efficient than padding all 1500 examples to 1024
data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    model=peft_model,
    padding="longest",
    label_pad_token_id=-100  # This is crucial for padding the labels
)

# 2. Update the trainer with the new data collator
trainer.data_collator = data_collator

print("Data collator updated successfully.")
print("The trainer is now ready for training.")

# Cell 9 - FIX: Enable Gradient Checkpointing

# This is the fix: We must explicitly enable gradient checkpointing on the
# PEFT-wrapped model.
peft_model.gradient_checkpointing_enable()

print("Gradient checkpointing enabled on the PEFT model.")

# Cell 10 - FIX: Disable Gradient Checkpointing

# This is the fix: We disable gradient checkpointing in the trainer arguments
# to resolve the 'RuntimeError: element 0 of tensors...'
trainer.args.gradient_checkpointing = False

print(f"Gradient checkpointing disabled in TrainingArguments.")

# Cell 11 - FIX: Correct Model & PEFT Setup
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# 1. Define the model ID
model_id = "deepseek-ai/deepseek-moe-16b-base"

# 2. Configure 4-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)

# 3. Load the 4-bit quantized model
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True
)
print("Model re-loaded.")

# 4. --- THIS IS THE CRITICAL FIX ---
# Prepare the k-bit model for training
# This automatically handles gradient checkpointing and other compatibility issues
model = prepare_model_for_kbit_training(model)
print("Model prepared for k-bit training.")

# 5. Define LoRA configuration
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
    ],
)

# 6. Apply LoRA to the prepared model
peft_model = get_peft_model(model, lora_config)

print("PEFT model re-configured.")
peft_model.print_trainable_parameters()

# Cell 12: Rebuild the Trainer

# 1. Use the TrainingArguments from our last successful attempt (FIX 5)
training_args = TrainingArguments(
    output_dir="./results-boolq",
    num_train_epochs=1,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    # Gradient Checkpointing is now handled by prepare_model_for_kbit_training
    # so we can set it to True here without the error.
    gradient_checkpointing=True,
    optim="paged_adamw_8bit",
    logging_steps=10,
    save_strategy="steps",
    save_steps=50,
    do_eval=True,
    eval_steps=50,
    learning_rate=2e-4,
    bf16=True,
    report_to="tensorboard", # Enable TensorBoard
)

# 2. Use the DataCollator from our previous fix (Cell 8 - FIX)
from transformers import DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    model=peft_model,
    padding="longest",
    label_pad_token_id=-100
)

# 3. Re-instantiate the Trainer
from transformers import Trainer
trainer = Trainer(
    model=peft_model,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    args=training_args,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

print("Trainer has been rebuilt with the correctly prepared model.")

# Cell 13: Start Fine-Tuning (Attempt 5)

print("--- Starting 1-Epoch Fine-Tuning for BoolQ (Final Attempt) ---")
print(f"Logging metrics to TensorBoard in './{training_args.output_dir}'")
print("This will take a few minutes...")

# This call will start the training and evaluation loop.
trainer.train()

print("\n--- Fine-Tuning Complete ---")

# Save the final adapter model
final_adapter_path = f"./{training_args.output_dir}/final-adapter"
trainer.save_model(final_adapter_path)

print(f"Final adapter model saved to {final_adapter_path}")

# Cell 15: Nuke Old Objects and Clear VRAM

import gc

# 1. Delete the old objects
del model
del peft_model
del trainer

# 2. Run the garbage collector and empty the cache
gc.collect()
torch.cuda.empty_cache()

# 3. Check the memory
allocated_gb = torch.cuda.memory_allocated(0) / (1024**3)
print(f"--- VRAM Cleared ---")
print(f"VRAM currently allocated: {allocated_gb:.2f} GB")

# Cell 16: Reload Model and Saved Adapter

from peft import PeftModel

# 1. Define model paths
model_id = "deepseek-ai/deepseek-moe-16b-base"
adapter_path = "./results-boolq/final-adapter" # The one you just saved

# 2. Configure 4-bit quantization (same as before)
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# 3. Load the base model
base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    device_map="auto",
    trust_remote_code=True
)

# 4. Load the PEFT adapter onto the base model
eval_model = PeftModel.from_pretrained(base_model, adapter_path)
eval_model.config.use_cache = False # Apply our earlier fix
eval_model.eval() # Put in evaluation mode

print(f"Successfully loaded base model and adapter from {adapter_path}")

# Cell 17 - THE REAL FIX: Manual Evaluation (Forward Pass)
import torch
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
import numpy as np

# 1. We re-use the DataLoader with batch_size=1
# (We need the data collator from the previous session)
eval_dataloader = DataLoader(
    tokenized_dataset["validation"],
    batch_size=1,  # The key fix for OOM
    collate_fn=data_collator
)

# 2. Get the metrics functions
accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")

# 3. Run the new manual loop
all_preds = []
all_labels = []

# (We need the eval_model from the previous session)
eval_model.eval()  # Put model in eval mode
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("--- Starting Manual Evaluation (Forward Pass, BS=1) ---")

# Use torch.no_grad() to save VRAM (no gradients)
with torch.no_grad():
    for batch in tqdm(eval_dataloader):
        # Move batch to GPU
        input_ids = batch["input_ids"].to(device)
        labels = batch["labels"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        # 1. --- THIS IS THE FIX ---
        # Do a standard forward pass.
        # This is NOT generate() and will not use the buggy cache.
        outputs = eval_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        # 2. Get the predictions (logits)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)

        # 3. Decode the predictions and labels
        # We only care about the parts where the label is not -100

        # Set ignored tokens to pad_token_id for safe decoding
        labels[labels == -100] = tokenizer.pad_token_id
        predictions[labels == tokenizer.pad_token_id] = tokenizer.pad_token_id

        # Decode the single example
        decoded_pred = tokenizer.decode(predictions[0], skip_special_tokens=True)
        decoded_label = tokenizer.decode(labels[0], skip_special_tokens=True)

        # 4. Clean and store the text
        all_preds.append("Yes" if "yes" in decoded_pred.lower() else "No")
        all_labels.append("Yes" if "yes" in decoded_label.lower() else "No")

# 5. Compute final metrics
print("\n--- Manual Evaluation Complete ---")
try:
    acc_results = accuracy_metric.compute(predictions=all_preds, references=all_labels)
    f1_results = f1_metric.compute(predictions=all_preds, references=all_labels, average="macro", pos_label="Yes")

    print(f"Eval Accuracy: {acc_results['accuracy']:.4f}")
    print(f"Eval F1: {f1_results['f1']:.4f}")

except Exception as e:
    print(f"Error computing metrics: {e}")
    print("Sample Preds:", all_preds[:10])
    print("Sample Labels:", all_labels[:10])

# Cell 18: Re-Calculate Metrics with Integers

# 1. The loop is already finished and all_preds/all_labels are in memory.
#    We just need to convert them from strings ("Yes"/"No") to integers (1/0).

preds_int = [1 if p == "Yes" else 0 for p in all_preds]
labels_int = [1 if l == "Yes" else 0 for l in all_labels]

print("--- Calculating Metrics (Fixed) ---")

# 2. Get the metrics functions (if they are not in memory, this is safe)
accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")

# 3. Compute metrics using the integer lists
try:
    acc_results = accuracy_metric.compute(predictions=preds_int, references=labels_int)

    # Use pos_label=1 (for "Yes") and the integer lists
    f1_results = f1_metric.compute(
        predictions=preds_int,
        references=labels_int,
        average="macro",
        pos_label=1
    )

    print("\n--- Final Evaluation Results ---")
    print(f"Eval Accuracy: {acc_results['accuracy']:.4f}")
    print(f"Eval F1: {f1_results['f1']:.4f}")

except Exception as e:
    print(f"Error computing metrics: {e}")
    print("Sample Preds (int):", preds_int[:10])
    print("Sample Labels (int):", labels_int[:10])

# Cell 20 (THE REAL FIX): Redefine MoERouterMonitor and Plotting Function
import torch
import json
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import os

class MoERouterMonitor:
    def __init__(self):
        self.step = 0
        self.entropy_history = []
        self.capacity_counts = None
        self.num_experts = None
        self._handles = []

    @staticmethod
    def _entropy(probs, dim=-1, eps=1e-9):
        p = probs.clamp_min(eps)
        return -(p * p.log()).sum(dim=dim)

    def _hook(self, name):
        def fn(module, inp, out):
            x = out[0] if isinstance(out, tuple) else out
            if x is None or not torch.is_tensor(x) or x.dim() < 2:
                return

            router_logits = x.float() # [*, 64]

            if self.num_experts is None:
                self.num_experts = router_logits.shape[-1] # This will now be 64
                self.capacity_counts = torch.zeros(self.num_experts, dtype=torch.long, device="cpu")

            probs = torch.softmax(router_logits, dim=-1)
            H = self._entropy(probs, dim=-1).mean().item()
            self.entropy_history.append((int(self.step), float(H)))

            top_k_indices = torch.topk(router_logits, k=2, dim=-1).indices.flatten()
            counts = torch.bincount(top_k_indices.to("cpu"), minlength=self.num_experts)
            self.capacity_counts[:len(counts)] += counts

        return fn

    def attach(self, model):
        # --- THIS IS THE REAL FIX ---
        # We build the *exact* list of 27 router names based on your inspection
        # (The PEFT model adds 'base_model.model.' to the start)
        self.target_names = [
            f"base_model.model.model.layers.{i}.mlp.gate" for i in range(1, 28)
        ]

        print("--- Attaching MoE Monitor (THE REAL FIX) ---")
        print(f"Attempting to hook {len(self.target_names)} exact router names...")
        count = 0

        # We create a dictionary of all modules for fast lookup
        module_lookup = dict(model.named_modules())

        for name in self.target_names:
            if name in module_lookup:
                try:
                    module = module_lookup[name]
                    self._handles.append(module.register_forward_hook(self._hook(name)))
                    count += 1
                except Exception as e:
                    print(f"  [FAILED] to attach to {name}: {e}")
            else:
                print(f"  [NOT FOUND] Could not find module: {name}")

        print(f"\n--- Attached to {count} router module(s) ---")
        if count != 27:
            print("\nCRITICAL WARNING: Did not hook all 27 routers. The names might be slightly different.")
            print("Please check the 'NOT FOUND' messages above.")
        else:
            print("Successfully hooked all 27 MoE routers.")

    def detach(self):
        for h in self._handles:
            try: h.remove()
            except: pass
        self._handles = []
        print("--- MoE Monitor detached ---")

    def tick(self):
        self.step += 1

# (This plotting function is unchanged and correct)
def save_router_plots_and_stats(out_dir, router_monitor):
    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)
    if router_monitor.entropy_history:
        xs = [s for s,_ in router_monitor.entropy_history]
        ys = [h for _,h in router_monitor.entropy_history]
        plt.figure(); plt.plot(xs, ys)
        plt.title("Routing Entropy (mean per token)"); plt.xlabel("Step"); plt.ylabel("Entropy")
        plt.tight_layout(); plt.savefig(out_dir/"routing_entropy.png"); plt.close()

    if router_monitor.capacity_counts is not None and router_monitor.num_experts:
        x = np.arange(router_monitor.num_experts)
        y = router_monitor.capacity_counts.numpy()
        total_calls = y.sum()
        y_percent = (y / total_calls) * 100 if total_calls > 0 else y

        plt.figure(figsize=(max(10, router_monitor.num_experts // 2), 6)) # Made wider for 64 experts
        plt.bar(x, y_percent)
        plt.title(f"Expert Capacity Utilization (Top-2, based on {total_calls} calls)")
        plt.xlabel("Expert ID"); plt.ylabel("Utilization (%)")
        plt.xticks(x, rotation=90, fontsize=8) # Rotate labels for 64 experts
        plt.tight_layout(); plt.savefig(out_dir/"expert_capacity.png"); plt.close()

    stats = {
        "entropy_history": router_monitor.entropy_history,
        "num_experts": router_monitor.num_experts,
        "capacity_counts": router_monitor.capacity_counts.tolist() if router_monitor.capacity_counts is not None else None
    }
    with open(out_dir/"router_stats.json","w") as f:
        json.dump(stats, f, indent=2)

    print(f"âœ… Routing plots and stats saved to: {out_dir}")

print("MoERouterMonitor class has been redefined with the REAL fix.")

# Cell 21 (RE-RUN): Run Evaluation with MoE Monitor Attached

# 1. Instantiate the (new, real) monitor
router_monitor = MoERouterMonitor()

# 2. Attach it to our existing evaluation model
try:
    router_monitor.attach(eval_model)
except NameError:
    print("ERROR: 'eval_model' is not in memory. Please re-run Cell 16 to load it.")

# 3. We re-use the DataLoader with batch_size=1
try:
    eval_dataloader
except NameError:
    print("Re-creating eval_dataloader...")
    eval_dataloader = DataLoader(
        tokenized_dataset["validation"],
        batch_size=1,
        collate_fn=data_collator
    )

# 4. Run the manual loop again (this populates the monitor)
eval_model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("--- Running Evaluation to Capture MoE Metrics (THE REAL ATTEMPT) ---")
print("This will take ~20-25 minutes...")

with torch.no_grad():
    for batch in tqdm(eval_dataloader, desc="Monitoring MoE Routing (THE REAL ONE)"):
        router_monitor.tick()
        input_ids = batch["input_ids"].to(device)
        labels = batch["labels"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        outputs = eval_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

# 5. Detach the monitor (cleanup)
router_monitor.detach()

print("\n--- MoE Data Capture Complete ---")
print(f"Monitor captured {len(router_monitor.entropy_history)} steps.")
print(f"Total expert calls recorded: {router_monitor.capacity_counts.sum().item() if router_monitor.capacity_counts is not None else 'N/A'}")

# Cell 22 (RE-RUN): Generate MoE Metrics and Plots
from IPython.display import Image, display

# 1. Define the output directory
moe_output_dir = "./results-boolq/moe-metrics-real"

# 2. Call the plotting function
print(f"--- Saving MoE Metrics to {moe_output_dir} ---")
try:
    save_router_plots_and_stats(moe_output_dir, router_monitor)

    print("\n--- Summary of Captured Metrics (THE REAL ONE) ---")
    print(f"Total Experts Detected: {router_monitor.num_experts}") # This should now be 64

    if router_monitor.entropy_history:
        ys = [h for _,h in router_monitor.entropy_history]
        mean_entropy = np.mean(ys)
        print(f"Mean Routing Entropy: {mean_entropy:.4f}")

except Exception as e:
    print(f"\nAn error occurred during plotting: {e}")

# 3. Display the new, correct plots
entropy_img = Path(moe_output_dir) / "routing_entropy.png"
capacity_img = Path(moe_output_dir) / "expert_capacity.png"

if entropy_img.exists():
    print("\n--- Routing Entropy Plot (THE REAL ONE) ---")
    display(Image(filename=str(entropy_img)))
else:
    print("\nCould not find routing_entropy.png")

if capacity_img.exists():
    print("\n--- Expert Capacity Utilization Plot (THE REAL ONE) ---")
    display(Image(filename=str(capacity_img)))
else:
    print("\nCould not find expert_capacity.png")