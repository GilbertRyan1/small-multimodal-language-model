# -*- coding: utf-8 -*-
"""Math.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hmPCO9iSMiE7fD_bT3ks2dPnFSvo7ha5
"""

!pip install -q transformers datasets accelerate bitsandbytes peft scikit-learn

import re
import torch
import numpy as np
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
)
from peft import PeftModel, prepare_model_for_kbit_training
from sklearn.metrics import accuracy_score

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", DEVICE)

# Base model
BASE_MODEL_ID = "deepseek-ai/deepseek-moe-16b-base"

# Path to your already-trained LOGIC adapter
LOGIC_ADAPTER_PATH = "/content/drive/MyDrive/logic_ai2_ar/final-adapter"

# Where to save the new Math (GSM8K) adapter
MATH_ADAPTER_PATH = "/content//drive/MyDrive/math_gsm8k_from_logic"

MAX_SEQ_LEN = 512

# Load GSM8K (main split)
gsm = load_dataset("gsm8k", "main")

train_gsm = gsm["train"]
test_gsm = gsm["test"]   # we’ll carve out a small val set from train to save time

print("GSM8K train size:", len(train_gsm))
print("GSM8K test size:", len(test_gsm))

def extract_final_answer(ans_text: str) -> str:
    """
    GSM8K answers are usually like:
    '... explanation ... #### 42'
    We take the part after '####' and strip spaces and commas.
    """
    if "####" in ans_text:
        ans = ans_text.split("####")[-1].strip()
    else:
        ans = ans_text.strip()
    # Normalize: remove trailing period, commas, spaces
    ans = ans.replace(",", "").strip()
    ans = ans.rstrip(".")
    return ans

# 1500 training examples, 200 validation examples
train_val_split = train_gsm.train_test_split(test_size=50, seed=42)
train_dataset = train_val_split["train"].select(range(1500))
val_dataset   = train_val_split["test"]

print(f"Train subset size: {len(train_dataset)}")
print(f"Val subset size: {len(val_dataset)}")

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def format_gsm8k_example(example):
    question = example["question"]
    full_answer = example["answer"]
    clean_answer = extract_final_answer(full_answer)

    prompt = (
        f"Question: {question}\n\n"
        f"Answer with only the final numeric answer.\n\n"
        f"Answer:"
    )
    full_text = f"{prompt} {clean_answer}"

    return prompt, full_text, clean_answer

def preprocess_gsm8k(examples):
    prompts = []
    full_texts = []
    answers = []

    for i in range(len(examples["question"])):
        ex = {k: v[i] for k, v in examples.items()}
        prompt, full_text, clean = format_gsm8k_example(ex)
        prompts.append(prompt)
        full_texts.append(full_text)
        answers.append(clean)

    # Tokenize prompt+answer for model input
    model_inputs = tokenizer(
        full_texts,
        max_length=MAX_SEQ_LEN,
        truncation=True,
        padding="max_length",
    )

    # Tokenize prompt alone to figure out which tokens to mask in labels
    prompt_inputs = tokenizer(
        prompts,
        max_length=MAX_SEQ_LEN,
        truncation=True,
        padding="max_length",
    )
    prompt_lengths = [
        int(np.sum(mask)) for mask in prompt_inputs["attention_mask"]
    ]

    labels = np.array(model_inputs["input_ids"])
    # Mask prompt tokens with -100
    for i, plen in enumerate(prompt_lengths):
        labels[i, :plen] = -100
    # Mask padding tokens
    labels[model_inputs["attention_mask"] == 0] = -100

    model_inputs["labels"] = labels.tolist()
    model_inputs["clean_answer"] = answers  # for later evaluation

    return model_inputs

tokenized_train = train_dataset.map(
    preprocess_gsm8k,
    batched=True,
    remove_columns=train_dataset.column_names,
)

tokenized_val = val_dataset.map(
    preprocess_gsm8k,
    batched=True,
    remove_columns=val_dataset.column_names,
)

print("Tokenized train example keys:", tokenized_train.column_names)

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=quant_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)
base_model.config.use_cache = False

# Prepare for k-bit training
base_model = prepare_model_for_kbit_training(base_model)

print("Loading LOGIC adapter from:", LOGIC_ADAPTER_PATH)
model = PeftModel.from_pretrained(
    base_model,
    LOGIC_ADAPTER_PATH,
    is_trainable=True,   # continue training this adapter on GSM8K
)
model.print_trainable_parameters()

training_args = TrainingArguments(
    output_dir=MATH_ADAPTER_PATH,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    num_train_epochs=1,
    learning_rate=2e-4,
    logging_steps=20,
    save_strategy="steps",
    save_steps=200,
    save_total_limit=2,
    eval_strategy="no",  # we will eval manually
    bf16=True,
    gradient_checkpointing=True,
    report_to="tensorboard",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=None,  # no automatic eval
)

print("--- Starting GSM8K fine-tuning from LOGIC adapter ---")
train_result = trainer.train()
print("--- GSM8K fine-tuning complete ---")

# Save the new adapter
print("Saving Math (GSM8K) adapter...")
trainer.save_model(MATH_ADAPTER_PATH)
tokenizer.save_pretrained(MATH_ADAPTER_PATH)

print("Saved Math adapter to:", MATH_ADAPTER_PATH)

!pip install -q transformers datasets accelerate bitsandbytes peft scikit-learn

import torch
import numpy as np
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel, prepare_model_for_kbit_training
from sklearn.metrics import accuracy_score
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import json
from pathlib import Path

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", DEVICE)

BASE_MODEL_ID = "deepseek-ai/deepseek-moe-16b-base"

LOGIC_ADAPTER_PATH = "/content/drive/MyDrive/logic_ai2_ar/final-adapter"
MATH_ADAPTER_PATH  = "/content/drive/MyDrive/math_gsm8k_from_logic"

MAX_SEQ_LEN = 512

# 1) Load GSM8K (math modality)
math_dataset = load_dataset("gsm8k", "main")
math_train = math_dataset["train"]
math_test  = math_dataset["test"]

print(f"Math train size (GSM8K): {len(math_train)}")
print(f"Math test size:          {len(math_test)}")

# For evaluation, we can optionally use a subset, e.g. first 200 problems
math_eval = math_test.select(range(50))
print(f"Math eval subset size:   {len(math_eval)}")

def format_math_prompt(example):
    """
    Build a simple math QA prompt.
    """
    question = example["question"]
    prompt = f"Question: {question}\nAnswer:"
    answer = example["answer"]  # full reasoning + final answer line in GSM8K
    return prompt, answer

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# 4-bit quantization for memory
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# Base DeepSeek model
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=quant_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)
base_model.config.use_cache = False
base_model = prepare_model_for_kbit_training(base_model)

# Step 1: load LOGIC adapter (previous modality)
print("Loading Logic adapter from:", LOGIC_ADAPTER_PATH)
model = PeftModel.from_pretrained(
    base_model,
    LOGIC_ADAPTER_PATH,
    is_trainable=False,
)

# Step 2: load MATH adapter (trained on top of Logic)
print("Loading Math adapter from:", MATH_ADAPTER_PATH)
model = PeftModel.from_pretrained(
    model,
    MATH_ADAPTER_PATH,
    is_trainable=False,
)

# Activate the math adapter by name (adjust if you used a different name)
try:
    model.set_adapter("math_gsm8k_from_logic")
except Exception:
    # If you aren't sure of the adapter name, you can inspect:
    print("Available adapters:", model.peft_config.keys())
    # Then manually set the correct one.

model.to(DEVICE)
model.eval()

print("Math evaluation model ready: Base + Logic adapter + Math adapter.")

print("\n=== Evaluating MATH modality (Exact Match, base + Logic + Math adapters) ===")

exact_match_count = 0
total_examples = 0
skipped = 0

for i, ex in enumerate(math_eval):
    prompt, answer = format_math_prompt(ex)
    full_text = prompt + " " + answer

    # Tokenize full text and prompt to find answer span
    enc_full = tokenizer(
        full_text,
        max_length=MAX_SEQ_LEN,
        truncation=True,
        padding=False,
        return_tensors="pt",
    )
    enc_prompt = tokenizer(
        prompt,
        max_length=MAX_SEQ_LEN,
        truncation=True,
        padding=False,
        return_tensors="pt",
    )

    input_ids = enc_full["input_ids"].to(DEVICE)         # [1, L]
    attention_mask = enc_full["attention_mask"].to(DEVICE)
    L = input_ids.shape[1]
    prompt_len = enc_prompt["input_ids"].shape[1]

    # Need at least one answer token and space for LM shift
    if L <= prompt_len + 1:
        skipped += 1
        continue

    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
        )
        logits = outputs.logits  # [1, L, vocab]

    # Check whether EVERY answer token is predicted correctly
    all_correct = True
    for t in range(prompt_len, L):
        true_id = input_ids[0, t].item()
        pred_logits = logits[0, t - 1, :]
        pred_id = int(torch.argmax(pred_logits))

        if pred_id != true_id:
            all_correct = False
            break

    total_examples += 1
    if all_correct:
        exact_match_count += 1

    if i < 3:
        print(f"\nExample {i}")
        print("Prompt:\n", prompt)
        print("Answer (truncated):", answer[:120].replace("\n", " "))
        print(f"Answer tokens in eval: {L - prompt_len}")
        print("Exact match for this example:", all_correct)

if total_examples == 0:
    math_exact_match = 0.0
else:
    math_exact_match = exact_match_count / total_examples

print("\n--- Math Results (Base + Logic + Math) ---")
print(f"Total evaluated examples: {total_examples}")
print(f"Exact matches:            {exact_match_count}")
print(f"Exact match accuracy:     {math_exact_match:.4f}")
print(f"Skipped examples (too short/truncated): {skipped}")

import matplotlib.pyplot as plt
import numpy as np
import json
from pathlib import Path
import torch

class MoERouterMonitor:
    def __init__(self):
        self.step = 0
        self.entropy_history = []
        self.capacity_counts = None
        self.num_experts = None
        self._handles = []

    @staticmethod
    def _entropy(probs, dim=-1, eps=1e-9):
        p = probs.clamp_min(eps)
        return -(p * p.log()).sum(dim=dim)

    def _hook(self, name):
        def fn(module, inp, out):
            x = out[0] if isinstance(out, tuple) else out
            if x is None or not torch.is_tensor(x) or x.dim() < 2:
                return

            router_logits = x.float()  # [..., num_experts]

            # Initialize counters the first time we see router logits
            if self.num_experts is None:
                self.num_experts = router_logits.shape[-1]
                self.capacity_counts = torch.zeros(
                    self.num_experts, dtype=torch.long, device="cpu"
                )

            probs = torch.softmax(router_logits, dim=-1)
            H = self._entropy(probs, dim=-1).mean().item()
            self.entropy_history.append((int(self.step), float(H)))

            # top-2 expert selection
            top_k_indices = torch.topk(router_logits, k=2, dim=-1).indices.flatten()
            counts = torch.bincount(
                top_k_indices.to("cpu"),
                minlength=self.num_experts
            )
            self.capacity_counts[:len(counts)] += counts
        return fn

    def attach(self, model):
        """Auto-discover and hook every module whose name ends with 'mlp.gate'."""
        print("--- Attaching MoE Monitor (auto-discovery) ---")
        count = 0
        for name, module in model.named_modules():
            if name.endswith("mlp.gate"):
                try:
                    h = module.register_forward_hook(self._hook(name))
                    self._handles.append(h)
                    count += 1
                    print(f"  [ATTACHED] {name}")
                except Exception as e:
                    print(f"  [FAILED] {name}: {e}")
        print(f"\nAttached to {count} router module(s).")
        if count == 0:
            print("⚠️  No 'mlp.gate' modules found — check model.named_modules().")

    def detach(self):
        for h in self._handles:
            try:
                h.remove()
            except:
                pass
        self._handles = []
        print("--- MoE Monitor detached ---")

    def tick(self):
        self.step += 1


def save_router_plots_and_stats(out_dir, router_monitor: MoERouterMonitor):
    """Save routing entropy curve, expert utilization bar chart, and raw JSON stats."""
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # 1️⃣  Routing entropy plot
    if router_monitor.entropy_history:
        xs = [s for s, _ in router_monitor.entropy_history]
        ys = [h for _, h in router_monitor.entropy_history]
        plt.figure()
        plt.plot(xs, ys)
        plt.title("Routing Entropy (mean per token)")
        plt.xlabel("Step")
        plt.ylabel("Entropy")
        plt.tight_layout()
        plt.savefig(out_dir / "routing_entropy.png")
        plt.close()

    # 2️⃣  Expert-capacity utilization plot
    if router_monitor.capacity_counts is not None and router_monitor.num_experts:
        x = np.arange(router_monitor.num_experts)
        y = router_monitor.capacity_counts.numpy()
        total_calls = y.sum()
        y_percent = (y / total_calls) * 100 if total_calls > 0 else y
        plt.figure(figsize=(max(10, router_monitor.num_experts // 2), 6))
        plt.bar(x, y_percent)
        plt.title(f"Expert Capacity Utilization (Top-2, {total_calls} calls)")
        plt.xlabel("Expert ID")
        plt.ylabel("Utilization (%)")
        plt.xticks(x, rotation=90, fontsize=8)
        plt.tight_layout()
        plt.savefig(out_dir / "expert_capacity.png")
        plt.close()

    # 3️⃣  Raw stats file
    stats = {
        "entropy_history": router_monitor.entropy_history,
        "num_experts": router_monitor.num_experts,
        "capacity_counts": (
            router_monitor.capacity_counts.tolist()
            if router_monitor.capacity_counts is not None else None
        ),
    }
    with open(out_dir / "router_stats.json", "w") as f:
        json.dump(stats, f, indent=2)

    print(f"✅ Routing plots and stats saved to: {out_dir}")

import torch
from datasets import Dataset
from torch.utils.data import DataLoader

# Tokenize math_eval for MoE monitoring
def preprocess_math_for_moe(examples): # Renamed 'example' to 'examples' to reflect batched input
    prompts = []
    for i in range(len(examples["question"])): # Iterate through the batch
        ex = {k: v[i] for k, v in examples.items()} # Extract a single example
        prompt, _ = format_math_prompt(ex)
        prompts.append(prompt)

    enc = tokenizer(
        prompts, # Tokenize the list of prompts
        max_length=MAX_SEQ_LEN,
        truncation=True,
        padding="max_length",
    )
    return enc

tokenized_math_eval = math_eval.map(
    preprocess_math_for_moe,
    batched=True,
    remove_columns=math_eval.column_names,
)
tokenized_math_eval.set_format(type="torch")

math_loader = DataLoader(
    tokenized_math_eval,
    batch_size=1,
    shuffle=False,
)

router_monitor = MoERouterMonitor()
router_monitor.attach(model)  # model = base + logic + math

print("\n--- Capturing MoE routing for MATH model ---")
with torch.no_grad():
    for batch in math_loader:
        router_monitor.tick()
        input_ids = batch["input_ids"].to(DEVICE)
        attention_mask = batch["attention_mask"].to(DEVICE)
        _ = model(input_ids=input_ids, attention_mask=attention_mask)

router_monitor.detach()

print("\nSteps captured:", len(router_monitor.entropy_history))
print("Total expert calls:",
      router_monitor.capacity_counts.sum().item()
      if router_monitor.capacity_counts is not None else "N/A")
print("Detected experts:", router_monitor.num_experts)

from IPython.display import Image, display

math_moe_output_dir = "/content/results/math_moe_with_logic_final"
print(f"--- Saving MATH MoE Metrics to {math_moe_output_dir} ---")

try:
    save_router_plots_and_stats(math_moe_output_dir, router_monitor)

    print("\n--- Summary of Captured MATH Metrics ---")
    print(f"Total Experts Detected: {router_monitor.num_experts}")

    if router_monitor.entropy_history:
        ys = [h for _, h in router_monitor.entropy_history]
        mean_entropy = np.mean(ys)
        print(f"Mean Routing Entropy: {mean_entropy:.4f}")

except Exception as e:
    print(f"\nAn error occurred during plotting: {e}")

entropy_img = Path(math_moe_output_dir) / "routing_entropy.png"
capacity_img = Path(math_moe_output_dir) / "expert_capacity.png"

if entropy_img.exists():
    print("\n--- Routing Entropy Plot (MATH) ---")
    display(Image(filename=str(entropy_img)))
else:
    print("\nCould not find routing_entropy.png")

if capacity_img.exists():
    print("\n--- Expert Capacity Utilization Plot (MATH) ---")
    display(Image(filename=str(capacity_img)))
else:
    print("\nCould not find expert_capacity.png")