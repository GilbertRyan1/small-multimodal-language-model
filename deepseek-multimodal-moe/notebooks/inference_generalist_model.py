# -*- coding: utf-8 -*-
"""Inference.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RywzF6zjm8jJ55zmtk-9heJhehFtoUi-
"""

!pip install -q transformers bitsandbytes peft

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
BASE_MODEL_ID = "deepseek-ai/deepseek-moe-16b-base"

# Map modality names → your adapter dirs
ADAPTER_PATHS = {
    "text":  "/content/drive/MyDrive/final-adapter",              # BoolQ
    "logic": "/content/drive/MyDrive/logic_ai2_ar/final-adapter",               # ARC-Challenge
    "math":  "/content/drive/MyDrive/math_gsm8k_from_logic",      # GSM8K
    "code":  "/content/drive/MyDrive/code_codeparrot",            # CodeParrot subset
    "image": "/content/drive/MyDrive/image_coco_from_code",       # COCO captions
    "video": "/content/drive/MyDrive/video_youcook2_from_image",  # YouCook2 segments
}

# 1) Tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# 2) Base model in 4-bit
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=quant_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)
base_model.config.use_cache = False

# 3) Wrap with the *first* adapter so we get a PeftModel
first_name, first_path = next(iter(ADAPTER_PATHS.items()))
print(f"Loading first adapter '{first_name}' from {first_path}")
model = PeftModel.from_pretrained(
    base_model,
    first_path,
    adapter_name=first_name,
    is_trainable=False,
)

# 4) Load remaining adapters into the same model
for name, path in list(ADAPTER_PATHS.items())[1:]:
    print(f"Loading adapter '{name}' from {path}")
    model.load_adapter(path, adapter_name=name)

model.to(DEVICE)
print("✅ Loaded adapters:", list(model.peft_config.keys()))

def set_modality(modality: str):
    if modality not in ADAPTER_PATHS:
        raise ValueError(f"Unknown modality '{modality}'. Available: {list(ADAPTER_PATHS.keys())}")
    model.set_adapter(modality)
    print(f"✅ Active adapter set to: {modality}")


def greedy_generate_no_cache(prompt: str, modality: str, max_new_tokens: int = 64):
    """
    Manual greedy decoding that calls model(...) directly,
    avoiding model.generate() and the DynamicCache bug.
    """
    set_modality(modality)

    # Encode prompt
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids = inputs["input_ids"].to(DEVICE)

    # We'll keep extending this tensor
    for _ in range(max_new_tokens):
        attn_mask = torch.ones_like(input_ids, device=DEVICE)

        with torch.no_grad():
            outputs = model(
                input_ids=input_ids,
                attention_mask=attn_mask,
                use_cache=False,   # make sure cache isn't used
            )

        # Take the last token's logits
        next_token_logits = outputs.logits[:, -1, :]  # [1, vocab]
        next_token_id = int(torch.argmax(next_token_logits, dim=-1).item())

        # Stop if EOS
        if next_token_id == tokenizer.eos_token_id:
            break

        # Append next token
        next_token = torch.tensor([[next_token_id]], device=DEVICE)
        input_ids = torch.cat([input_ids, next_token], dim=1)

    # Decode **only the new part** or the whole sequence; here we do full for simplicity
    return tokenizer.decode(input_ids[0], skip_special_tokens=True)


# --- Examples using the new greedy_generate_no_cache ---

print("\n=== TEXT example ===")
text_prompt = "Question: Is water wet?\n\nAnswer:"
print(greedy_generate_no_cache(text_prompt, modality="text", max_new_tokens=10))

print("\n=== LOGIC example ===")
logic_prompt = (
    "Question: 2, 4, 8, 16, ?\n"
    "Choices:\n(A) 18\n(B) 20\n(C) 24\n(D) 32\n\n"
    "Answer:"
)
print(greedy_generate_no_cache(logic_prompt, modality="logic", max_new_tokens=4))

print("\n=== MATH example ===")
math_prompt = "Solve: If 3x + 5 = 20, what is x?\n\nAnswer:"
print(greedy_generate_no_cache(math_prompt, modality="math", max_new_tokens=32))

print("\n=== CODE example ===")
code_prompt = "Write a Python function that returns the factorial of n.\n\n```python\n"
print(greedy_generate_no_cache(code_prompt, modality="code", max_new_tokens=64))

!pip install -q pillow torchvision

import os
import torch
import torch.nn as nn
from transformers import CLIPVisionModel, AutoImageProcessor

# --- Shared vision backbone (CLIP) ---
VISION_MODEL_ID = "openai/clip-vit-base-patch32"
image_processor = AutoImageProcessor.from_pretrained(VISION_MODEL_ID)

vision_model = CLIPVisionModel.from_pretrained(VISION_MODEL_ID).to(DEVICE)
for p in vision_model.parameters():
    p.requires_grad = False

vision_hidden_size = vision_model.config.hidden_size
lm_hidden_size = model.base_model.model.model.embed_tokens.embedding_dim

PREFIX_LENGTH = 10   # must match training
NUM_FRAMES = 4       # must match video training


# --- IMAGE prefix projector (re-create same arch as in training) ---
class ImagePrefixProjector(nn.Module):
    def __init__(self, vision_dim, lm_dim, prefix_len):
        super().__init__()
        self.prefix_len = prefix_len
        self.lm_dim = lm_dim
        self.proj = nn.Sequential(
            nn.Linear(vision_dim, lm_dim * prefix_len),
            nn.Tanh(),
        )
    def forward(self, vision_emb):
        x = self.proj(vision_emb)
        return x.view(-1, self.prefix_len, self.lm_dim)

image_prefix_projector = ImagePrefixProjector(
    vision_dim=vision_hidden_size,
    lm_dim=lm_hidden_size,
    prefix_len=PREFIX_LENGTH,
).to(DEVICE)

image_prefix_ckpt = os.path.join(ADAPTER_PATHS["image"], "prefix_projector.pt")
assert os.path.exists(image_prefix_ckpt), f"Missing {image_prefix_ckpt}"
image_prefix_projector.load_state_dict(
    torch.load(image_prefix_ckpt, map_location=DEVICE)
)


# --- VIDEO temporal encoder + prefix projector (same as training) ---
class TemporalEncoder(nn.Module):
    def __init__(self, vision_dim, num_frames, num_layers=1, num_heads=8):
        super().__init__()
        self.num_frames = num_frames
        self.vision_dim = vision_dim
        enc_layer = nn.TransformerEncoderLayer(
            d_model=vision_dim,
            nhead=num_heads,
            dim_feedforward=vision_dim * 4,
            batch_first=True,
        )
        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=num_layers)
        self.pos_embed = nn.Parameter(torch.zeros(1, num_frames, vision_dim))

    def forward(self, frame_pixel_values):
        """
        frame_pixel_values: [B, T, C, H, W]
        """
        B, T, C, H, W = frame_pixel_values.shape
        x = frame_pixel_values.view(B * T, C, H, W)
        with torch.no_grad():
            vo = vision_model(pixel_values=x)
        if hasattr(vo, "pooler_output") and vo.pooler_output is not None:
            frame_emb = vo.pooler_output
        else:
            frame_emb = vo.last_hidden_state[:, 0, :]
        x = frame_emb.view(B, T, self.vision_dim) + self.pos_embed
        x = self.transformer(x)
        return x.mean(dim=1)  # [B, D]


class VideoPrefixProjector(nn.Module):
    def __init__(self, vision_dim, lm_dim, prefix_len):
        super().__init__()
        self.prefix_len = prefix_len
        self.lm_dim = lm_dim
        self.proj = nn.Sequential(
            nn.Linear(vision_dim, lm_dim * prefix_len),
            nn.Tanh(),
        )
    def forward(self, video_emb):
        x = self.proj(video_emb)
        return x.view(-1, self.prefix_len, self.lm_dim)

temporal_encoder = TemporalEncoder(
    vision_dim=vision_hidden_size,
    num_frames=NUM_FRAMES,
).to(DEVICE)

video_prefix_projector = VideoPrefixProjector(
    vision_dim=vision_hidden_size,
    lm_dim=lm_hidden_size,
    prefix_len=PREFIX_LENGTH,
).to(DEVICE)

video_temporal_ckpt = os.path.join(ADAPTER_PATHS["video"], "temporal_encoder.pt")
video_prefix_ckpt   = os.path.join(ADAPTER_PATHS["video"], "video_prefix_projector.pt")

assert os.path.exists(video_temporal_ckpt), f"Missing {video_temporal_ckpt}"
assert os.path.exists(video_prefix_ckpt),   f"Missing {video_prefix_ckpt}"

temporal_encoder.load_state_dict(
    torch.load(video_temporal_ckpt, map_location=DEVICE)
)
video_prefix_projector.load_state_dict(
    torch.load(video_prefix_ckpt, map_location=DEVICE)
)

print("✅ Vision + image & video heads loaded.")

from PIL import Image

class MultimodalDeepSeek:
    def __init__(
        self,
        model,
        tokenizer,
        vision_model,
        image_processor,
        image_prefix_projector,
        temporal_encoder,
        video_prefix_projector,
        device="cuda",
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.vision_model = vision_model
        self.image_processor = image_processor
        self.image_prefix_projector = image_prefix_projector
        self.temporal_encoder = temporal_encoder
        self.video_prefix_projector = video_prefix_projector
        self.device = device

    def _set_adapter(self, name: str):
        self.model.set_adapter(name)

    # -------- TEXT MODALITIES --------
    def generate_text(self, prompt, max_new_tokens=64):
        self._set_adapter("text")
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            out = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                pad_token_id=self.tokenizer.eos_token_id,
                use_cache=False
            )
        return self.tokenizer.decode(out[0], skip_special_tokens=True)

    def generate_logic(self, prompt, max_new_tokens=4):
        self._set_adapter("logic")
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            out = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                pad_token_id=self.tokenizer.eos_token_id,
                use_cache=False
            )
        return self.tokenizer.decode(out[0], skip_special_tokens=True)

    def generate_math(self, prompt, max_new_tokens=128):
        self._set_adapter("math")
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            out = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                pad_token_id=self.tokenizer.eos_token_id,
                use_cache=False
            )
        return self.tokenizer.decode(out[0], skip_special_tokens=True)

    def generate_code(self, prompt, max_new_tokens=128):
        self._set_adapter("code")
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            out = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                pad_token_id=self.tokenizer.eos_token_id,
                use_cache=False
            )
        return self.tokenizer.decode(out[0], skip_special_tokens=True)

    # -------- IMAGE CAPTIONING --------
    def caption_image(self, pil_image: Image.Image, max_new_tokens=32):
        self._set_adapter("image")

        # 1) Image → CLIP embedding
        vision_inputs = self.image_processor(
            images=pil_image.convert("RGB"),
            return_tensors="pt"
        ).to(self.device)

        with torch.no_grad():
            vo = self.vision_model(**vision_inputs)
        if hasattr(vo, "pooler_output") and vo.pooler_output is not None:
            img_emb = vo.pooler_output
        else:
            img_emb = vo.last_hidden_state[:, 0, :]

        prefix = self.image_prefix_projector(img_emb)   # [1, P, H]
        # Cast prefix to bfloat16 to match the base model's compute dtype
        prefix = prefix.to(torch.bfloat16)

        # 2) Greedy decode in LM embedding space
        lm = self.model
        start_id = (
            self.tokenizer.bos_token_id
            if self.tokenizer.bos_token_id is not None
            else (self.tokenizer.pad_token_id or self.tokenizer.eos_token_id)
        )
        generated = [start_id]

        for _ in range(max_new_tokens):
            input_ids = torch.tensor([generated], device=self.device)
            emb_layer = lm.base_model.model.model.embed_tokens
            text_embeds = emb_layer(input_ids)
            # Ensure text_embeds are also bfloat16
            text_embeds = text_embeds.to(torch.bfloat16)

            inputs_embeds = torch.cat([prefix, text_embeds], dim=1)
            # Ensure the final concatenated embeddings are in bfloat16
            inputs_embeds = inputs_embeds.to(torch.bfloat16)
            attn_mask = torch.ones(
                1,
                prefix.shape[1] + text_embeds.shape[1],
                dtype=torch.long,
                device=self.device,
            )

            with torch.no_grad():
                out = lm(inputs_embeds=inputs_embeds, attention_mask=attn_mask)
            next_id = int(torch.argmax(out.logits[0, -1, :]))

            if next_id == self.tokenizer.eos_token_id:
                break
            generated.append(next_id)

        return self.tokenizer.decode(generated, skip_special_tokens=True)

    # -------- VIDEO CAPTIONING --------
    def caption_video_frames(self, frames, max_new_tokens=32):
        """
        frames: list of PIL.Image frames from a clip (already sampled).
        """
        self._set_adapter("video")

        # 1) Preprocess frames with CLIP processor
        vision_inputs = self.image_processor(
            images=[f.convert("RGB") for f in frames],
            return_tensors="pt",
        )
        frame_pixel_values = vision_inputs["pixel_values"].unsqueeze(0).to(self.device)  # [1, T, C, H, W]

        # 2) Temporal encoding + prefix
        with torch.no_grad():
            video_emb = self.temporal_encoder(frame_pixel_values)
            prefix = self.video_prefix_projector(video_emb)  # [1, P, H]
        # Cast prefix to bfloat16 to match the base model's compute dtype
        prefix = prefix.to(torch.bfloat16)

        # 3) Greedy decode in LM space
        lm = self.model
        start_id = (
            self.tokenizer.bos_token_id
            if self.tokenizer.bos_token_id is not None
            else (self.tokenizer.pad_token_id or self.tokenizer.eos_token_id)
        )
        generated = [start_id]

        for _ in range(max_new_tokens):
            input_ids = torch.tensor([generated], device=self.device)
            emb_layer = lm.base_model.model.model.embed_tokens
            text_embeds = emb_layer(input_ids)
            # Ensure text_embeds are also bfloat16
            text_embeds = text_embeds.to(torch.bfloat16)

            inputs_embeds = torch.cat([prefix, text_embeds], dim=1)
            # Ensure the final concatenated embeddings are in bfloat16
            inputs_embeds = inputs_embeds.to(torch.bfloat16)
            attn_mask = torch.ones(
                1,
                prefix.shape[1] + text_embeds.shape[1],
                dtype=torch.long,
                device=self.device,
            )

            with torch.no_grad():
                out = lm(inputs_embeds=inputs_embeds, attention_mask=attn_mask)
            next_id = int(torch.argmax(out.logits[0, -1, :]))

            if next_id == self.tokenizer.eos_token_id:
                break
            generated.append(next_id)

        return self.tokenizer.decode(generated, skip_special_tokens=True)


# Instantiate the generalist model
multi_model = MultimodalDeepSeek(
    model=model,
    tokenizer=tokenizer,
    vision_model=vision_model,
    image_processor=image_processor,
    image_prefix_projector=image_prefix_projector,
    temporal_encoder=temporal_encoder,
    video_prefix_projector=video_prefix_projector,
    device=DEVICE,
)

print("✅ MultimodalDeepSeek generalist wrapper ready.")

from PIL import Image
import cv2
import numpy as np

# --- TEXT / LOGIC / MATH / CODE ---

print("\n=== TEXT example ===")
print(multi_model.generate_text(
    "Question: Is the Eiffel Tower in Paris?\n\nAnswer:",
    max_new_tokens=10
))

print("\n=== LOGIC example ===")
print(multi_model.generate_logic(
    "Question: 1, 1, 2, 3, 5, ?\nChoices:\n(A) 7\n(B) 8\n(C) 9\n(D) 11\n\nAnswer:",
    max_new_tokens=4
))

print("\n=== MATH example ===")
print(multi_model.generate_math(
    "Solve: A train travels 120 km in 3 hours. What is its speed in km/h?\n\nAnswer:",
    max_new_tokens=32
))

print("\n=== CODE example ===")
print(multi_model.generate_code(
    "Write a Python function that checks if a number is prime.\n\n```python\n",
    max_new_tokens=64
))


# --- IMAGE CAPTIONING EXAMPLE ---

# Change this to any local image path in your Colab/Drive
image_path = "/content/sample.jpg"  # <-- replace with real image
if os.path.exists(image_path):
    pil_img = Image.open(image_path)
    print("\n=== IMAGE example ===")
    caption = multi_model.caption_image(pil_img, max_new_tokens=32)
    print("Image caption:", caption)
else:
    print("\n[IMAGE example skipped] Set 'image_path' to a real image file.")


# --- VIDEO CAPTIONING EXAMPLE ---

def sample_frames_from_video_file(path, num_frames=4):
    cap = cv2.VideoCapture(path)
    if not cap.isOpened():
        raise RuntimeError(f"Cannot open video: {path}")
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    if frame_count <= 0:
        cap.release()
        raise RuntimeError("Video has no frames")

    indices = np.linspace(0, frame_count - 1, num_frames, dtype=int)
    frames = []
    for idx in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))
        ret, frame = cap.read()
        if ret:
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(Image.fromarray(frame))
    cap.release()

    # Ensure we always return exactly num_frames
    if not frames:
        raise RuntimeError(f"Could not read any frames from video: {path}")

    while len(frames) < num_frames:
        frames.append(frames[-1]) # Pad by duplicating the last valid frame

    return frames

video_path = "//content/AUTHENTIC Fattoush Salad Recipe (Fattoush Recipe Video) [xHr8X2Wpmno].mp4"  # <-- replace with a real clip
if os.path.exists(video_path):
    frames = sample_frames_from_video_file(video_path, num_frames=NUM_FRAMES)
    print("\n=== VIDEO example ===")
    v_caption = multi_model.caption_video_frames(frames, max_new_tokens=32)
    print("Video caption:", v_caption)
else:
    print("\n[VIDEO example skipped] Set 'video_path' to a real mp4 file.")