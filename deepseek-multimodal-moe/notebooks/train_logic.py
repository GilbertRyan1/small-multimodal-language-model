# -*- coding: utf-8 -*-
"""Logic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QI2kwy9pCphQfhCj9jZFxDs30j4VoKx9
"""

!pip install -q transformers datasets accelerate bitsandbytes peft matplotlib seaborn tensorboard scikit-learn

import torch
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from datasets import load_dataset
from sklearn.metrics import accuracy_score, f1_score
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    TrainerCallback
)
from peft import (
    get_peft_model,
    PeftModel,
    LoraConfig,
    TaskType,
    prepare_model_for_kbit_training  # <-- IMPORT for FIX 1
)

# --- Configuration ---
BASE_MODEL_ID = "deepseek-ai/deepseek-moe-16b-base"
# !!! Make sure this path matches where you re-uploaded your adapter !!!
BOOLQ_ADAPTER_PATH = "/content/drive/MyDrive/final-adapter"
NEW_ADAPTER_PATH = "/content/results/logic_ai2_ar"
DATASET_ID = "ai2_arc"
DATASET_CONFIG = "ARC-Challenge"

# --- Load Dataset ---
dataset = load_dataset(DATASET_ID, DATASET_CONFIG)

# Split the dataset
train_val_split = dataset['train'].train_test_split(test_size=200, seed=42)
train_dataset = train_val_split['train']
val_dataset = train_val_split['test']

print(f"Training data: {len(train_dataset)} examples")
print(f"Validation data: {len(val_dataset)} examples")

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
tokenizer.pad_token = tokenizer.eos_token

def format_prompt(example):
    """Formats the AI2 AR example into a prompt."""
    question = example['question']
    choices_text = []

    labels = example['choices']['label']
    texts = example['choices']['text']

    choice_map = {}
    new_labels = [chr(65 + i) for i in range(len(labels))] # A, B, C, D...

    formatted_choices = []
    for i, (original_label, text) in enumerate(zip(labels, texts)):
        new_label = new_labels[i]
        choice_map[original_label] = new_label
        formatted_choices.append(f"({new_label}) {text}")

    choices_str = "\n".join(formatted_choices)

    correct_answer_label = choice_map[example['answerKey']]

    prompt = f"Question: {question}\n\nChoices:\n{choices_str}\n\nAnswer:"
    full_text = f"{prompt} {correct_answer_label}"

    return prompt, full_text, correct_answer_label

def preprocess_function(examples):
    prompts = []
    full_texts = []
    labels_list = []

    for i in range(len(examples['question'])):
        example = {k: v[i] for k, v in examples.items()}
        prompt, full_text, label = format_prompt(example)
        prompts.append(prompt)
        full_texts.append(full_text)
        labels_list.append(label)

    model_inputs = tokenizer(full_texts, max_length=512, truncation=True, padding="max_length")
    prompt_inputs = tokenizer(prompts, max_length=512, truncation=True, padding="max_length")
    prompt_lengths = [np.sum(mask) for mask in prompt_inputs['attention_mask']]

    labels = np.array(model_inputs['input_ids'])
    labels_mask = np.arange(labels.shape[1]) < np.array(prompt_lengths)[:, None]
    labels[labels_mask] = -100
    labels[model_inputs['attention_mask'] == 0] = -100

    model_inputs["labels"] = labels.tolist()
    model_inputs["clean_labels"] = labels_list

    return model_inputs

# Apply preprocessing
tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)
tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)

# Setup 4-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load base model
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=quantization_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)
model.config.use_cache = False

# --- FIX 1: This patches the model to work with gradient checkpointing ---
model = prepare_model_for_kbit_training(model)
# -----------------------------------------------------------------------

# --- Load and chain the BoolQ Adapter ---
print(f"Loading base model complete. Now loading adapter from: {BOOLQ_ADAPTER_PATH}")
model = PeftModel.from_pretrained(model, BOOLQ_ADAPTER_PATH, is_trainable=True)
print("BoolQ adapter loaded and set to trainable.")

# Define new LoRA config for this task
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
)

# Add the *new* adapter for the logic task
model.add_adapter("logic_ai2_ar", lora_config)
model.set_adapter("logic_ai2_ar")
print("New adapter 'logic_ai2_ar' added and set as active.")

model.print_trainable_parameters()

class MoERouterMonitor(TrainerCallback):
    """
    A callback to monitor and record MoE router statistics during training.
    It hooks into the model's forward pass to capture 'router_logits'.
    """
    def __init__(self):
        self.stats = []
        self.router_logits_handle = None

    def _capture_router_logits(self, model, args, output):
        """Hook function to capture router outputs."""
        if hasattr(output, 'router_logits') and output.router_logits:
            self.current_router_logits.append(
                output.router_logits[-1].clone().detach().cpu()
            )

    def on_evaluate_begin(self, args, state, control, model, **kwargs):
        """Register the forward hook before evaluation starts."""
        self.current_router_logits = []
        self.router_logits_handle = model.register_forward_hook(self._capture_router_logits)

    def on_evaluate_end(self, args, state, control, **kwargs):
        """Remove the hook and process the collected logits."""
        if self.router_logits_handle:
            self.router_logits_handle.remove()
            self.router_logits_handle = None

        if not self.current_router_logits:
            print("MoERouterMonitor: No router_logits captured.")
            return

        all_logits = torch.cat(self.current_router_logits, dim=0)
        num_tokens, num_experts = all_logits.shape

        gating_weights, selected_experts = torch.topk(all_logits, 2, dim=-1)
        gating_weights = torch.softmax(gating_weights, dim=-1, dtype=torch.float32)

        expert_counts = torch.zeros(num_experts, dtype=torch.long)
        for i in range(num_experts):
            expert_counts[i] = (selected_experts == i).sum()

        expert_utilization = expert_counts.float() / num_tokens

        p = expert_utilization
        p = p[p > 0]
        routing_entropy = (-torch.sum(p * torch.log(p))).item()

        cv_load = (expert_counts.float().std() / expert_counts.float().mean()).item()

        step_stats = {
            "step": state.global_step,
            "expert_utilization": expert_utilization.numpy(),
            "routing_entropy": routing_entropy,
            "load_balancing_cv": cv_load,
            "avg_gating_weight_top1": gating_weights[:, 0].mean().item(),
            "avg_gating_weight_top2": gating_weights[:, 1].mean().item(),
        }
        self.stats.append(step_stats)

        if not hasattr(state, 'moe_monitor_stats'):
            state.moe_monitor_stats = []
        state.moe_monitor_stats.append(step_stats)
        print(f"MoERouterMonitor: Step {state.global_step}, Entropy: {routing_entropy:.4f}, CV Load: {cv_load:.4f}")

    def get_stats(self):
        return self.stats

def save_router_plots_and_stats(stats_list, output_dir):
    """Saves plots and a CSV of the MoE router stats."""
    if not stats_list:
        print("No MoE stats to save.")
        return

    import os
    os.makedirs(output_dir, exist_ok=True) # Ensure directory exists

    df = pd.DataFrame(stats_list)

    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    sns.lineplot(data=df, x='step', y='routing_entropy', marker='o')
    plt.title('Routing Entropy over Training')
    plt.ylabel('Entropy')

    plt.subplot(1, 2, 2)
    sns.lineplot(data=df, x='step', y='load_balancing_cv', marker='o')
    plt.title('Load Balancing (CV) over Training')
    plt.ylabel('Coefficient of Variation (Load)')
    plt.tight_layout()
    plt.savefig(f"{output_dir}/moe_scalar_metrics.png")
    plt.close()

    utilization_data = np.array([s['expert_utilization'] for s in stats_list])
    steps = [s['step'] for s in stats_list]
    num_experts = utilization_data.shape[1]

    plt.figure(figsize=(15, 8))
    sns.heatmap(utilization_data.T, xticklabels=steps, yticklabels=[f'E{i}' for i in range(num_experts)])
    plt.title('Expert Utilization (Token Throughput %)')
    plt.xlabel('Training Step')
    plt.ylabel('Expert ID')
    plt.savefig(f"{output_dir}/moe_expert_utilization_heatmap.png")
    plt.close()

    df_exploded = df.explode('expert_utilization')
    df_exploded['expert_id'] = df_exploded.groupby(level=0).cumcount()
    df_exploded.to_csv(f"{output_dir}/moe_router_stats_full.csv", index=False)

    print(f"MoE router plots and stats saved to {output_dir}")

class MoETrainer(Trainer):
    """
    A custom trainer for DeepSeek-MoE that includes the router auxiliary loss
    (z-loss) in the total loss calculation.
    """
    # --- FIX for TypeError ---
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        outputs = model(**inputs)
        loss = outputs.loss

        if hasattr(outputs, 'router_losses') and outputs.router_losses:
            router_loss = 0
            for r_loss in outputs.router_losses:
                if r_loss is not None:
                    router_loss += r_loss
            loss += router_loss

        return (loss, outputs) if return_outputs else loss

def compute_metrics(eval_preds):
    """Computes F1 and Accuracy for the multiple-choice task."""
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)

    true_labels = []
    pred_labels = []

    for i in range(len(labels)):
        label_indices = np.where(labels[i] != -100)[0]
        if len(label_indices) > 0:
            idx = label_indices[0]
            true_labels.append(labels[i, idx])
            pred_labels.append(predictions[i, idx])

    if not true_labels:
        return {"accuracy": 0.0, "f1": 0.0}

    accuracy = accuracy_score(true_labels, pred_labels)
    f1 = f1_score(true_labels, pred_labels, average='macro')

    return {
        "accuracy": accuracy,
        "f1": f1
    }

# --- Training Arguments (New Strategy) ---
training_args = TrainingArguments(
    output_dir=NEW_ADAPTER_PATH,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,

    # --- YOUR STRATEGY: No evaluation during training ---
    eval_strategy="no",

    # --- OOM FIXES (for manual eval in Step 7) ---
    per_device_eval_batch_size=1,
    eval_accumulation_steps=10,

    num_train_epochs=1,
    learning_rate=2e-4,
    logging_steps=10,
    save_strategy="steps", # Still save checkpoints
    save_steps=50,
    bf16=True,
    gradient_checkpointing=True, # Enabled thanks to Step 3 fix
    report_to="tensorboard",
    save_total_limit=2,
)

# --- Initialize Trainer and Monitor ---
moe_monitor = MoERouterMonitor()

trainer = MoETrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,
    # --- FIX for Warning (tokenizer removed) ---
    compute_metrics=compute_metrics,
    callbacks=[moe_monitor]
)

# --- Start Training ---
print("Starting training from scratch (no in-training evaluation)...")

# This will run all 115 steps without interruption
# We are starting from scratch, so no resume_from_checkpoint
trainer.train()

print("--- Full Training Complete ---")

# --- Save Final Adapter ---
print("Saving final adapter.")
trainer.save_model(f"{NEW_ADAPTER_PATH}/final-adapter")
tokenizer.save_pretrained(f"{NEW_ADAPTER_PATH}/final-adapter")

print("Model training and saving complete. Ready for evaluation in the next cell.")

!pip install -q "transformers" "datasets" "accelerate" "bitsandbytes" "peft" "scikit-learn"

import torch
import numpy as np
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel, prepare_model_for_kbit_training
from sklearn.metrics import accuracy_score, f1_score
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import json
from pathlib import Path

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", DEVICE)

BASE_MODEL_ID = "deepseek-ai/deepseek-moe-16b-base"

# IMPORTANT: paths in *your* notebook
BOOLQ_ADAPTER_PATH = "/content/drive/MyDrive/final-adapter"               # previous (text) adapter
LOGIC_ADAPTER_PATH = "/content/drive/MyDrive/logic_ai2_ar/final-adapter"  # logic adapter you just trained

MAX_SEQ_LEN = 512

# Load AI2 ARC dataset (same split as training)
dataset = load_dataset("ai2_arc", "ARC-Challenge")
split = dataset["train"].train_test_split(test_size=50, seed=42)
logic_train = split["train"]
logic_val   = split["test"]

print(f"Logic train size: {len(logic_train)}")
print(f"Logic val size:   {len(logic_val)}")

def format_prompt(example):
    question = example["question"]
    labels = example["choices"]["label"]
    texts = example["choices"]["text"]

    choice_map = {}
    new_labels = [chr(65 + i) for i in range(len(labels))]  # A,B,C,D,...
    formatted_choices = []

    for i, (orig_label, t) in enumerate(zip(labels, texts)):
        new_label = new_labels[i]
        choice_map[orig_label] = new_label
        formatted_choices.append(f"({new_label}) {t}")

    choices_str = "\n".join(formatted_choices)
    correct = choice_map[example["answerKey"]]

    prompt = (
        f"Question: {question}\n\n"
        f"Choices:\n{choices_str}\n\n"
        f"Answer:"
    )

    return prompt, new_labels, correct

# Tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# 4-bit quantization
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# Base DeepSeek model
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=quant_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)
base_model.config.use_cache = False
base_model = prepare_model_for_kbit_training(base_model)

# STEP 1: load the previous (BoolQ/text) adapter
print("Loading previous (BoolQ/text) adapter from:", BOOLQ_ADAPTER_PATH)
model = PeftModel.from_pretrained(
    base_model,
    BOOLQ_ADAPTER_PATH,
    is_trainable=False,
)

# STEP 2: load the Logic adapter on top of that model
print("Loading Logic adapter from:", LOGIC_ADAPTER_PATH)
# This call loads an additional adapter into the same PEFT model
model = PeftModel.from_pretrained(
    model,
    LOGIC_ADAPTER_PATH,
    is_trainable=False,
)

# The logic adapter was created with name "logic_ai2_ar" in your training code
model.load_adapter(LOGIC_ADAPTER_PATH, adapter_name="logic_ai2_ar")
model.set_adapter("logic_ai2_ar")


model.to(DEVICE)
model.eval()

print("Model for Logic evaluation: Base + BoolQ adapter + Logic adapter.")

print("\n=== Evaluating Logic modality (with previous BoolQ adapter active) ===")

y_true, y_pred = [], []

for i, ex in enumerate(logic_val):
    prompt, label_list, correct = format_prompt(ex)
    y_true.append(correct)

    scores = []
    for letter in label_list:
        candidate = f"{prompt} {letter}"
        enc = tokenizer(
            candidate,
            max_length=MAX_SEQ_LEN,
            truncation=True,
            return_tensors="pt",
        ).to(DEVICE)

        with torch.no_grad():
            out = model(**enc)
            logits = out.logits[0, -1, :]  # final position

        letter_id = tokenizer(letter, add_special_tokens=False)["input_ids"][0]
        score = logits[letter_id].item()
        scores.append(score)

    best_idx = int(np.argmax(scores))
    pred = label_list[best_idx]
    y_pred.append(pred)

    if i < 3:
        print(f"\nExample {i}")
        print("Prompt:\n", prompt)
        print("Scores:", {l: s for l, s in zip(label_list, scores)})
        print("True:", correct, "Pred:", pred)

acc = accuracy_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred, average="macro", labels=["A","B","C","D"])

print("\n--- Logic Results (Base + BoolQ + Logic) ---")
print(f"Accuracy: {acc:.4f}")
print(f"Macro F1: {f1:.4f}")

import matplotlib.pyplot as plt
import numpy as np
import json
from pathlib import Path
import torch

class MoERouterMonitor:
    def __init__(self):
        self.step = 0
        self.entropy_history = []
        self.capacity_counts = None
        self.num_experts = None
        self._handles = []

    @staticmethod
    def _entropy(probs, dim=-1, eps=1e-9):
        p = probs.clamp_min(eps)
        return -(p * p.log()).sum(dim=dim)

    def _hook(self, name):
        def fn(module, inp, out):
            x = out[0] if isinstance(out, tuple) else out
            if x is None or not torch.is_tensor(x) or x.dim() < 2:
                return

            # x is router logits [..., num_experts]
            router_logits = x.float()

            # Initialize expert counters
            if self.num_experts is None:
                self.num_experts = router_logits.shape[-1]
                self.capacity_counts = torch.zeros(
                    self.num_experts, dtype=torch.long, device="cpu"
                )

            probs = torch.softmax(router_logits, dim=-1)
            H = self._entropy(probs, dim=-1).mean().item()
            self.entropy_history.append((int(self.step), float(H)))

            # top-2 experts per token
            top_k_indices = torch.topk(router_logits, k=2, dim=-1).indices.flatten()
            counts = torch.bincount(
                top_k_indices.to("cpu"),
                minlength=self.num_experts
            )
            self.capacity_counts[:len(counts)] += counts

        return fn

    def attach(self, model):
        """
        Auto-discover gate modules by name pattern: we hook any module
        whose name ends with 'mlp.gate'.
        """
        print("--- Attaching MoE Monitor (auto-discovery) ---")
        count = 0

        for name, module in model.named_modules():
            if name.endswith("mlp.gate"):
                try:
                    h = module.register_forward_hook(self._hook(name))
                    self._handles.append(h)
                    count += 1
                    print(f"  [ATTACHED] {name}")
                except Exception as e:
                    print(f"  [FAILED] {name}: {e}")

        print(f"\nAttached to {count} router module(s).")
        if count == 0:
            print("WARNING: No 'mlp.gate' modules found. Check model.named_modules().")

    def detach(self):
        for h in self._handles:
            try:
                h.remove()
            except:
                pass
        self._handles = []
        print("--- MoE Monitor detached ---")

    def tick(self):
        self.step += 1


def save_router_plots_and_stats(out_dir, router_monitor: MoERouterMonitor):
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # 1) Routing entropy plot
    if router_monitor.entropy_history:
        xs = [s for s, _ in router_monitor.entropy_history]
        ys = [h for _, h in router_monitor.entropy_history]
        plt.figure()
        plt.plot(xs, ys)
        plt.title("Routing Entropy (mean per token)")
        plt.xlabel("Step")
        plt.ylabel("Entropy")
        plt.tight_layout()
        plt.savefig(out_dir / "routing_entropy.png")
        plt.close()

    # 2) Expert capacity utilization
    if router_monitor.capacity_counts is not None and router_monitor.num_experts:
        x = np.arange(router_monitor.num_experts)
        y = router_monitor.capacity_counts.numpy()
        total_calls = y.sum()
        y_percent = (y / total_calls) * 100 if total_calls > 0 else y

        plt.figure(figsize=(max(10, router_monitor.num_experts // 2), 6))
        plt.bar(x, y_percent)
        plt.title(f"Expert Capacity Utilization (Top-2, {total_calls} calls)")
        plt.xlabel("Expert ID")
        plt.ylabel("Utilization (%)")
        plt.xticks(x, rotation=90, fontsize=8)
        plt.tight_layout()
        plt.savefig(out_dir / "expert_capacity.png")
        plt.close()

    stats = {
        "entropy_history": router_monitor.entropy_history,
        "num_experts": router_monitor.num_experts,
        "capacity_counts": (
            router_monitor.capacity_counts.tolist()
            if router_monitor.capacity_counts is not None else None
        ),
    }
    with open(out_dir / "router_stats.json", "w") as f:
        json.dump(stats, f, indent=2)

    print(f"âœ… Routing plots and stats saved to: {out_dir}")

from torch.utils.data import DataLoader

def preprocess_for_moe(example):
    enc = tokenizer(
        example["question"],
        max_length=MAX_SEQ_LEN,
        truncation=True,
        padding="max_length",
    )
    return enc

tokenized_logic_val = logic_val.map(
    preprocess_for_moe,
    batched=True,
    remove_columns=logic_val.column_names,
)
tokenized_logic_val.set_format(type="torch")

logic_loader = DataLoader(tokenized_logic_val, batch_size=1, shuffle=False)

router_monitor = MoERouterMonitor()
router_monitor.attach(model)  # 'model' = base + previous adapter + logic adapter

print("\n--- Capturing MoE routing for Logic model ---")
with torch.no_grad():
    for batch in logic_loader:
        router_monitor.tick()
        input_ids = batch["input_ids"].to(DEVICE)
        attention_mask = batch["attention_mask"].to(DEVICE)
        _ = model(input_ids=input_ids, attention_mask=attention_mask)

router_monitor.detach()

print("\nSteps captured:", len(router_monitor.entropy_history))
print("Total expert calls:",
      router_monitor.capacity_counts.sum().item()
      if router_monitor.capacity_counts is not None else "N/A")
print("Detected experts:", router_monitor.num_experts)

from IPython.display import Image, display
from pathlib import Path

logic_moe_output_dir = "/content/drive/MyDrive/logic_ai2_ar/moe-metrics-logic_final"  # or any path you like
print(f"--- Saving LOGIC MoE Metrics to {logic_moe_output_dir} ---")

try:
    save_router_plots_and_stats(logic_moe_output_dir, router_monitor)

    print("\n--- Summary of Captured LOGIC Metrics ---")
    print(f"Total Experts Detected: {router_monitor.num_experts}")

    if router_monitor.entropy_history:
        ys = [h for _, h in router_monitor.entropy_history]
        mean_entropy = np.mean(ys)
        print(f"Mean Routing Entropy: {mean_entropy:.4f}")

except Exception as e:
    print(f"\nAn error occurred during plotting: {e}")

entropy_img = Path(logic_moe_output_dir) / "routing_entropy.png"
capacity_img = Path(logic_moe_output_dir) / "expert_capacity.png"

if entropy_img.exists():
    print("\n--- Routing Entropy Plot (LOGIC) ---")
    display(Image(filename=str(entropy_img)))
else:
    print("\nCould not find routing_entropy.png")

if capacity_img.exists():
    print("\n--- Expert Capacity Utilization Plot (LOGIC) ---")
    display(Image(filename=str(capacity_img)))
else:
    print("\nCould not find expert_capacity.png")