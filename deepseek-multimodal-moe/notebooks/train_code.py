# -*- coding: utf-8 -*-
"""Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mgDZUyK8_z4Pzj_NYwA1bvaleV1OvYY3
"""

!pip install -q transformers datasets accelerate bitsandbytes peft scikit-learn

import torch
import numpy as np
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
)
from peft import (
    PeftModel,
    LoraConfig,
    TaskType,
    prepare_model_for_kbit_training,
)
from sklearn.metrics import accuracy_score

# -------------------
# PATHS & BASIC CONFIG
# -------------------
BASE_MODEL_ID = "deepseek-ai/deepseek-moe-16b-base"

# math-finetuned model (adapter) you already have
MATH_ADAPTER_PATH = "/content/drive/MyDrive/math_gsm8k_from_logic"

# where to save the new CODE adapter
CODE_ADAPTER_PATH = "/content/drive/MyDrive/code_codeparrot"

# CodeParrot dataset (clean valid split has 'content' field with code)
CODE_DATASET_ID = "codeparrot/codeparrot-clean-valid"

TRAIN_SAMPLES = 1500
EVAL_SAMPLES = 50
MAX_SEQ_LEN = 512

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", DEVICE)

# Load CodeParrot clean valid
raw_code = load_dataset(CODE_DATASET_ID, split="train")

print("Total rows in CodeParrot-clean-valid:", len(raw_code))
print("Columns:", raw_code.column_names)

# Subsample to fit your resource constraints
train_code = raw_code.select(range(TRAIN_SAMPLES))
eval_code  = raw_code.select(range(TRAIN_SAMPLES, TRAIN_SAMPLES + EVAL_SAMPLES))

print("Train subset size:", len(train_code))
print("Eval subset size:", len(eval_code))

# Load tokenizer from base DeepSeek model
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def preprocess_code_examples(examples):
    # Code is in the 'content' field
    texts = examples["content"]
    tokenized = tokenizer(
        texts,
        max_length=MAX_SEQ_LEN,
        truncation=True,
        padding="max_length",
    )
    # For plain LM, labels = input_ids (no masking)
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

tokenized_train_code = train_code.map(
    preprocess_code_examples,
    batched=True,
    remove_columns=train_code.column_names,
)

tokenized_eval_code = eval_code.map(
    preprocess_code_examples,
    batched=True,
    remove_columns=eval_code.column_names,
)

print(tokenized_train_code[0].keys())

# 4-bit quantization config
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# Load base DeepSeek model
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=quant_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)
base_model.config.use_cache = False

# Prepare for k-bit training
base_model = prepare_model_for_kbit_training(base_model)

# Load math adapter (frozen or lightly trainable base)
print(f"Loading math adapter from: {MATH_ADAPTER_PATH}")
model = PeftModel.from_pretrained(
    base_model,
    MATH_ADAPTER_PATH,
    is_trainable=False,  # we freeze math adapter weights
)

# Define LoRA config for CODE
code_lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ],
)

# Add a new adapter for CODE on top of math
model.add_adapter("code_codeparrot", code_lora_config)
model.set_adapter("code_codeparrot")

print("Adapters:")
model.print_trainable_parameters()

# Simple Trainer (we don't include custom router loss here to keep it simple;
# you can reuse your MoETrainer if you want, it will also work)

training_args = TrainingArguments(
    output_dir=CODE_ADAPTER_PATH,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    eval_strategy="no",   # no eval during training (we do it manually)
    num_train_epochs=1,
    learning_rate=2e-4,
    logging_steps=10,
    save_strategy="steps",
    save_steps=100,
    bf16=True,
    gradient_checkpointing=True,
    report_to="tensorboard",           # or "tensorboard" if you use it
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_code,
    eval_dataset=tokenized_eval_code,  # just for convenience; we won't call trainer.evaluate()
    tokenizer=tokenizer,
)

print("Trainer for CODE modality is ready.")

print("=== Starting CODE fine-tuning on CodeParrot subset ===")
train_result = trainer.train()
print("=== Training complete ===")

# Save only the CODE adapter
print("Saving CODE adapter to:", CODE_ADAPTER_PATH)
trainer.save_model(CODE_ADAPTER_PATH)          # saves PEFT adapter weights
tokenizer.save_pretrained(CODE_ADAPTER_PATH)   # save tokenizer alongside

print("CODE adapter saved.")

!pip install -q transformers datasets accelerate bitsandbytes peft scikit-learn

import torch
import numpy as np
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel, prepare_model_for_kbit_training

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", DEVICE)

# Paths
BASE_MODEL_ID = "deepseek-ai/deepseek-moe-16b-base"
MATH_ADAPTER_PATH = "/content/drive/MyDrive/math_gsm8k_from_logic"
CODE_ADAPTER_PATH = "/content/drive/MyDrive/code_codeparrot"

CODE_DATASET_ID = "codeparrot/codeparrot-clean-valid"
MAX_SEQ_LEN = 512
EVAL_SAMPLES = 50

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load CodeParrot and take eval subset (same way as during training)
raw_code = load_dataset(CODE_DATASET_ID, split="train")
print("Total CodeParrot rows:", len(raw_code))

# We assume you used first 1500 for train, next 200 for eval
eval_code = raw_code.select(range(1500, 1500 + EVAL_SAMPLES))
print("Eval subset size:", len(eval_code))

import torch
import numpy as np
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel, prepare_model_for_kbit_training

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", DEVICE)

# Paths
BASE_MODEL_ID = "deepseek-ai/deepseek-moe-16b-base"
MATH_ADAPTER_PATH = "/content/drive/MyDrive/math_gsm8k_from_logic"
CODE_ADAPTER_PATH = "/content/drive/MyDrive/code_codeparrot"

CODE_DATASET_ID = "codeparrot/codeparrot-clean-valid"
MAX_SEQ_LEN = 512
EVAL_SAMPLES = 50

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load CodeParrot and take eval subset (same way as during training)
raw_code = load_dataset(CODE_DATASET_ID, split="train")
print("Total CodeParrot rows:", len(raw_code))

# We assume you used first 1500 for train, next 200 for eval
eval_code = raw_code.select(range(1500, 1500 + EVAL_SAMPLES))
print("Eval subset size:", len(eval_code))

# 4-bit quantization config
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# Load base model (DeepSeek-MoE) in 4-bit
base_model_eval = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    quantization_config=quant_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True,
)
base_model_eval.config.use_cache = False

# Prepare for k-bit (even though we won't train, it's safe)
base_model_eval = prepare_model_for_kbit_training(base_model_eval)

# Load math adapter
print("Loading math adapter from:", MATH_ADAPTER_PATH)
eval_model = PeftModel.from_pretrained(
    base_model_eval,
    MATH_ADAPTER_PATH,
    adapter_name="math_adapter", # Explicitly name the math adapter
    is_trainable=False,
)

# Load CODE adapter on top of math
print("Loading CODE adapter from:", CODE_ADAPTER_PATH)
eval_model.load_adapter(CODE_ADAPTER_PATH, adapter_name="code_codeparrot") # Use load_adapter to add it to the existing PeftModel

# Make sure the CODE adapter is active (name must match what you used in training)
eval_model.set_adapter("code_codeparrot")

eval_model.to(DEVICE)
eval_model.eval()

print("Eval model with math + code adapters is ready on", DEVICE)

from sklearn.metrics import accuracy_score

def compute_pass_at_1_last_token(model, tokenizer, dataset, max_seq_len=MAX_SEQ_LEN):
    model.eval()
    pass_count = 0
    total = 0

    for ex in dataset:
        code = ex["content"]
        enc = tokenizer(
            code,
            max_length=max_seq_len,
            truncation=True,
            padding=False,
            return_tensors="pt",
        )

        input_ids = enc["input_ids"].to(DEVICE)      # [1, L]
        attention_mask = enc["attention_mask"].to(DEVICE)

        seq_len = input_ids.shape[1]
        if seq_len < 2:
            continue  # need at least one prompt token and one to predict

        # Prompt is all but the last token; target is last token
        prompt_ids = input_ids[:, :-1]
        target_id = input_ids[0, -1]
        prompt_mask = attention_mask[:, :-1]

        with torch.no_grad():
            outputs = model(
                input_ids=prompt_ids,
                attention_mask=prompt_mask,
            )
            # logits: [1, L-1, vocab]; prediction for final position
            logits = outputs.logits[0, -1, :]
            pred_id = torch.argmax(logits).item()

        total += 1
        if pred_id == target_id.item():
            pass_count += 1

    if total == 0:
        return 0.0
    return pass_count / total

print("=== Evaluating approximate Pass@1 (last-token completion) on eval set ===")
approx_pass_at_1 = compute_pass_at_1_last_token(
    eval_model,
    tokenizer,
    eval_code,
    max_seq_len=MAX_SEQ_LEN,
)

print(f"Approximate Pass@1 (last-token completion): {approx_pass_at_1:.4f}")

from torch.utils.data import DataLoader
import torch

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
eval_model.to(DEVICE)
eval_model.eval()

MAX_SEQ_LEN = 512  # same as you used for code training

def code_data_collator(batch):
    # batch is a list of dicts from eval_code, each with "content"
    texts = [ex["content"] for ex in batch]
    enc = tokenizer(
        texts,
        max_length=MAX_SEQ_LEN,
        truncation=True,
        padding="max_length",
        return_tensors="pt",
    )
    return {
        "input_ids": enc["input_ids"],
        "attention_mask": enc["attention_mask"],
    }

code_eval_loader = DataLoader(
    eval_code,
    batch_size=1,      # memory-safe and fine for routing stats
    shuffle=False,
    collate_fn=code_data_collator,
)

print("CODE eval DataLoader for MoE monitoring is ready.")

import torch
import json
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import os

class MoERouterMonitor:
    def __init__(self):
        self.step = 0
        self.entropy_history = []
        self.capacity_counts = None
        self.num_experts = None
        self._handles = []

    @staticmethod
    def _entropy(probs, dim=-1, eps=1e-9):
        p = probs.clamp_min(eps)
        return -(p * p.log()).sum(dim=dim)

    def _hook(self, name):
        def fn(module, inp, out):
            x = out[0] if isinstance(out, tuple) else out
            if x is None or not torch.is_tensor(x) or x.dim() < 2:
                return

            router_logits = x.float()

            if self.num_experts is None:
                self.num_experts = router_logits.shape[-1]
                self.capacity_counts = torch.zeros(self.num_experts, dtype=torch.long, device="cpu")

            probs = torch.softmax(router_logits, dim=-1)
            H = self._entropy(probs, dim=-1).mean().item()
            self.entropy_history.append((int(self.step), float(H)))

            top_k_indices = torch.topk(router_logits, k=2, dim=-1).indices.flatten()
            counts = torch.bincount(top_k_indices.to("cpu"), minlength=self.num_experts)
            self.capacity_counts[:len(counts)] += counts

        return fn

    def attach(self, model):
        self.target_names = [f"base_model.model.model.layers.{i}.mlp.gate" for i in range(1, 28)]
        print("--- Attaching MoE Monitor (CODE) ---")
        module_lookup = dict(model.named_modules())
        count = 0
        for name in self.target_names:
            if name in module_lookup:
                try:
                    module = module_lookup[name]
                    self._handles.append(module.register_forward_hook(self._hook(name)))
                    count += 1
                except Exception as e:
                    print(f"  [FAILED] to attach to {name}: {e}")
            else:
                print(f"  [NOT FOUND] {name}")
        print(f"Attached to {count} router modules")

    def detach(self):
        for h in self._handles:
            try:
                h.remove()
            except:
                pass
        self._handles = []
        print("--- MoE Monitor detached ---")

    def tick(self):
        self.step += 1


def save_router_plots_and_stats(out_dir, router_monitor):
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    if router_monitor.entropy_history:
        xs = [s for s, _ in router_monitor.entropy_history]
        ys = [h for _, h in router_monitor.entropy_history]
        plt.figure()
        plt.plot(xs, ys)
        plt.title("Routing Entropy (mean per token)")
        plt.xlabel("Step")
        plt.ylabel("Entropy")
        plt.tight_layout()
        plt.savefig(out_dir / "routing_entropy.png")
        plt.close()

    if router_monitor.capacity_counts is not None and router_monitor.num_experts:
        x = np.arange(router_monitor.num_experts)
        y = router_monitor.capacity_counts.numpy()
        total_calls = y.sum()
        y_percent = (y / total_calls) * 100 if total_calls > 0 else y
        plt.figure(figsize=(max(10, router_monitor.num_experts // 2), 6))
        plt.bar(x, y_percent)
        plt.title(f"Expert Capacity Utilization (Top-2, {total_calls} calls)")
        plt.xlabel("Expert ID")
        plt.ylabel("Utilization (%)")
        plt.tight_layout()
        plt.savefig(out_dir / "expert_capacity.png")
        plt.close()

    stats = {
        "entropy_history": router_monitor.entropy_history,
        "num_experts": router_monitor.num_experts,
        "capacity_counts": router_monitor.capacity_counts.tolist() if router_monitor.capacity_counts is not None else None,
    }
    with open(out_dir / "router_stats.json", "w") as f:
        json.dump(stats, f, indent=2)

    print(f"âœ… Routing plots and stats saved to {out_dir}")

from tqdm.auto import tqdm
import torch

# 1. Instantiate monitor
code_router_monitor = MoERouterMonitor()

# 2. Attach to current eval_model (with code adapter active)
code_router_monitor.attach(eval_model)

# 3. Run forward passes to collect routing stats
print("\n--- Running CODE MoE Monitoring ---")

with torch.no_grad():
    for batch in tqdm(code_eval_loader, desc="Monitoring MoE Routing (CODE)"):
        code_router_monitor.tick()

        input_ids = batch["input_ids"].to(DEVICE)
        attention_mask = batch["attention_mask"].to(DEVICE)

        _ = eval_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
        )

# 4. Detach monitor
code_router_monitor.detach()

print("\n--- CODE MoE Data Capture Complete ---")
print(f"Steps captured: {len(code_router_monitor.entropy_history)}")
print(
    "Total expert calls recorded:",
    code_router_monitor.capacity_counts.sum().item()
    if code_router_monitor.capacity_counts is not None
    else "N/A",
)
print("Detected experts:", code_router_monitor.num_experts)

from IPython.display import Image, display
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt # Import matplotlib for plotting

# Define the helper function for saving plots and stats
def save_router_plots_and_stats(output_dir, monitor):
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # Plot Routing Entropy
    if monitor.entropy_history:
        steps, entropy_values = zip(*monitor.entropy_history)
        plt.figure(figsize=(10, 5))
        plt.plot(steps, entropy_values, marker='o', linestyle='-')
        plt.title('Routing Entropy Over Time (CODE)')
        plt.xlabel('Step')
        plt.ylabel('Entropy')
        plt.grid(True)
        plt.savefig(output_path / "routing_entropy.png")
        plt.close()

    # Plot Expert Capacity Utilization
    if monitor.capacity_counts is not None:
        expert_counts = monitor.capacity_counts.cpu().numpy()
        num_experts = len(expert_counts)
        plt.figure(figsize=(12, 6))
        plt.bar(range(num_experts), expert_counts)
        plt.title('Expert Capacity Utilization (CODE)')
        plt.xlabel('Expert Index')
        plt.ylabel('Total Tokens Processed')
        plt.xticks(range(num_experts))
        plt.grid(axis='y')
        plt.savefig(output_path / "expert_capacity.png")
        plt.close()

# 1. Output dir for CODE MoE metrics
code_moe_output_dir = f"{CODE_ADAPTER_PATH}/moe-metrics-code"
print(f"--- Saving CODE MoE Metrics to {code_moe_output_dir} ---")

try:
    save_router_plots_and_stats(code_moe_output_dir, code_router_monitor)

    print("\n--- Summary of Captured CODE Metrics ---")
    print(f"Total Experts Detected: {code_router_monitor.num_experts}")

    if code_router_monitor.entropy_history:
        ys = [h for _, h in code_router_monitor.entropy_history]
        mean_entropy = np.mean(ys)
        print(f"Mean Routing Entropy: {mean_entropy:.4f}")

except Exception as e:
    print(f"\nAn error occurred during plotting: {e}")

# 2. Display plots if they exist
entropy_img = Path(code_moe_output_dir) / "routing_entropy.png"
capacity_img = Path(code_moe_output_dir) / "expert_capacity.png"

if entropy_img.exists():
    print("\n--- CODE Routing Entropy Plot ---")
    display(Image(filename=str(entropy_img)))
else:
    print("\nCould not find routing_entropy.png")

if capacity_img.exists():
    print("\n--- CODE Expert Capacity Utilization Plot ---")
    display(Image(filename=str(capacity_img)))
else:
    print("\nCould not find expert_capacity.png")